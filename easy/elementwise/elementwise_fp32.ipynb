{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81732b5",
   "metadata": {},
   "source": [
    "# CUDA入门\n",
    "\n",
    "本文为入门CUDA的简单知识点的整理。\n",
    "\n",
    "## 线程模型\n",
    "\n",
    "GPU上的计算由大量的线程组成，因此有了三层的抽象概念——线程、线程块和线程块网络。\n",
    "\n",
    "> thread -> block -> grid\n",
    "\n",
    "计算最小的执行单元是一个线程，很多线程组成一个线程块，很多线程块组成一个线程块网格。\n",
    "\n",
    "CUDA中有内置变量帮助我们识别线程在计算当中的位置，例如：\n",
    "\n",
    "- `threadIdx.x`表示线程在一个线程块内的一维编号\n",
    "- `blockIdx.x`表示当前的线程块在线程块网格中的编号\n",
    "- `blockDim.x`表示在一个线程块中有多少个线程\n",
    "- `gridDim.x`表示一个线程块网络中有多少个线程块\n",
    "- `blockIdx.x * blockDim.x + threadIdx.x`计算出一个线程在全局中的唯一索引\n",
    "\n",
    "## 内存模型\n",
    "\n",
    "CUDA的内存模型和计算机的存储系统设计思想一样，为了速度采用分层的思想。\n",
    "\n",
    "- 寄存器：最快，每个线程私有\n",
    "- 共享内存：一个线程块中的所有线程共享，速度快，适合块内通信\n",
    "- 全局内存：最慢，但是所有的线程都可以访问，容量大\n",
    "\n",
    "## 核函数\n",
    "\n",
    "核函数（Kernel Funtion）就是在GPU上运行的函数，通常用`__global__`声明，例如：\n",
    "\n",
    "```c\n",
    "__global__ void kernel(float* A, float* B, float* C, int N) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < N) {\n",
    "        C[idx] = A[idx] + B[idx];\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "在调用时应遵循如下格式：\n",
    "\n",
    "```c\n",
    "kernel<<<blocks, threads_per_block>>>(A, B, C, N);\n",
    "```\n",
    "\n",
    "## Pytorch的C++/CUDA拓展\n",
    "\n",
    "CUDA内核和PyTorch中的张量结合，需要用到PyTorch的C++ API。\n",
    "\n",
    "## PyBind\n",
    "\n",
    "PyBind11是一个C++11的库，用来把C++写的类/函数导出为Python模块。\n",
    "\n",
    "例如，如果我们使用C++写了一个加法函数：\n",
    "\n",
    "```cpp\n",
    "int add (int x, int y) {\n",
    "    return x + y\n",
    "}\n",
    "```\n",
    "\n",
    "如果我们要在Python中使用，必须要借助PyBind，应该这样写：\n",
    "\n",
    "```cpp\n",
    "#include <pybind11/pybind11.h>\n",
    "namespace py = pybind11;\n",
    "\n",
    "int add(int a, int b) {\n",
    "    return a + b;\n",
    "}\n",
    "\n",
    "// 暴露给 Python\n",
    "PYBIND11_MODULE(example, m) {\n",
    "    m.def(\"add\", &add, \"A function that adds two numbers\");\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "## PyTorch的C++拓展\n",
    "\n",
    "PyTorch的C++拓展就是基于PyBind11的，需要我们使用封装好的库：\n",
    "\n",
    "```cpp\n",
    "#include <torch/extension.h>\n",
    "```\n",
    "\n",
    "然后导入：\n",
    "\n",
    "```cpp\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"add\", &add_cuda, \"Elementwise Add (CUDA)\");\n",
    "}\n",
    "```\n",
    "\n",
    "需要深入理解C++与Python的语法特性，充分发挥两者的优势，在实际开发中实现高效的优化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d147f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile elementwise_fp32.cu\n",
    "#include <torch/extension.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "__global__ void elementwise_add_f32_kernel(const float* A, const float* B, float* C, int N) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < N) {\n",
    "        C[idx] = A[idx] + B[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void elementwise_add_f32(torch::Tensor A, torch::Tensor B, torch::Tensor C) {\n",
    "    int N = A.numel();\n",
    "    int threads = 256;\n",
    "    int blocks = (N + threads - 1) / threads;\n",
    "    elementwise_add_f32_kernel<<<blocks, threads>>>(\n",
    "        A.data_ptr<float>(),\n",
    "        B.data_ptr<float>(),\n",
    "        C.data_ptr<float>(),\n",
    "        N\n",
    "    );\n",
    "}\n",
    "\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"elementwise_add_f32\", &elementwise_add_f32, \"Elementwise Add f32\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8207c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/shaneyale/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/shaneyale/.cache/torch_extensions/py312_cu124/elementwise_lib/build.ninja...\n",
      "/home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module elementwise_lib...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output elementwise_fp32.cuda.o.d -DTORCH_EXTENSION_NAME=elementwise_lib -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/include -isystem /home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/include/TH -isystem /home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /home/shaneyale/miniconda3/envs/cu124/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 -std=c++17 -c /home/shaneyale/codes/cuda/easy/elementwise/elementwise_fp32.cu -o elementwise_fp32.cuda.o \n",
      "[2/2] c++ elementwise_fp32.cuda.o -shared -L/home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.4/lib64 -lcudart -o elementwise_lib.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module elementwise_lib...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time per call: 0.018693 ms\n",
      "Time per call: 0.014354 ms\n",
      "验证结果是否一致: True\n"
     ]
    }
   ],
   "source": [
    "# 测试代码如下\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# 编译并加载 CUDA 扩展\n",
    "lib = load(\n",
    "    name=\"elementwise_lib\",\n",
    "    sources=[\"elementwise_fp32.cu\"],\n",
    "    extra_cuda_cflags=[\"-O3\"],\n",
    "    extra_cflags=[\"-std=c++17\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def run_benchmark(func, a, b, out=None, iters=1000):\n",
    "    # warmup\n",
    "    for _ in range(10):\n",
    "        func(a, b, out)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(iters):\n",
    "        func(a, b, out)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    mean_time = (end - start) * 1000 / iters\n",
    "    print(f\"Time per call: {mean_time:.6f} ms\")\n",
    "\n",
    "# 测试数据\n",
    "N = 1024 * 1024  # 一百万个元素\n",
    "a = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n",
    "b = torch.randn(N, device=\"cuda\", dtype=torch.float32)\n",
    "c = torch.zeros_like(a)\n",
    "\n",
    "# 跑自定义 kernel\n",
    "run_benchmark(lib.elementwise_add_f32, a, b, c)\n",
    "\n",
    "# 跑 PyTorch 内置加法\n",
    "run_benchmark(lambda x,y,z: torch.add(x,y,out=z), a, b, c)\n",
    "\n",
    "# 对比结果\n",
    "print(\"验证结果是否一致:\", torch.allclose(a+b, c))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
