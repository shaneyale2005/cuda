{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8788405c",
   "metadata": {},
   "source": [
    "# float32x4向量加法\n",
    "\n",
    "## 任务理解\n",
    "\n",
    "CUDA中的float4表示4个float组成的一组数据，本任务是对于两个形状相同的浮点矩阵A和B进行逐元素加法，一次读取和运算4个元素，得到C。\n",
    "\n",
    "## 具体实现\n",
    "\n",
    "CUDA将一个block内的线程组织为三维结构，方便处理多维数据。\n",
    "\n",
    "在CUDA代码：\n",
    "```c\n",
    "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "```\n",
    "这里用来获取在x方向上的索引，我们把矩阵展开成一个一维数组进行处理。\n",
    "\n",
    "注意CUDA kenel的调用方法，内核名称之后要跟着<<<blocks, threads>>>。\n",
    "\n",
    "在C++中，`reinterpret_cast`是一个类型转换操作符，不改变指针指向的内存，只是告诉编译器“把这个类型当做是另外一种类型来看待”。\n",
    "\n",
    "`data_ptr<T>()`返回一个张量首地址的原始指针，这是一个模版函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fd11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting elementwise_fp32x4.cu\n"
     ]
    }
   ],
   "source": [
    "%%writefile elementwise_fp32x4.cu\n",
    "#include <torch/extension.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// float4向量化的kernel\n",
    "__global__ void elementwise_add_f32x4_kernel (const float4 *A, const float4 *B, float4 *C, int N4) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < N4) {\n",
    "        float4 va = A[idx];\n",
    "        float4 vb = B[idx];\n",
    "        C[idx] = make_float4(va.x + vb.x, va.y + vb.y, va.z + vb.z, va.w + vb.w);\n",
    "    }\n",
    "}\n",
    "\n",
    "void elementwise_add_f32x4 (torch::Tensor A, torch::Tensor B, torch::Tensor C) {\n",
    "    // numel()函数用来获取A中的元素总数\n",
    "    int N = A.numel();\n",
    "    int N4 = N / 4;\n",
    "    int threads = 256;\n",
    "    // 下面这个计算其实就是向上取整，数学有点意思\n",
    "    int blocks = (N4 + threads - 1) / threads;\n",
    "\n",
    "    // 主体部分按照float4来进行处理\n",
    "    elementwise_add_f32x4_kernel<<<blocks, threads>>>(\n",
    "        reinterpret_cast<const float4*>(A.data_ptr<float>()),\n",
    "        reinterpret_cast<const float4*>(B.data_ptr<float>()),\n",
    "        reinterpret_cast<float4*>(C.data_ptr<float>()),\n",
    "        N4\n",
    "    );\n",
    "\n",
    "    // 尾部处理\n",
    "    int remain = N % 4;\n",
    "    if (remain > 0) {\n",
    "        int start = N - remain;\n",
    "        // 这里会自动推导为float*\n",
    "        auto A_ptr = A.data_ptr<float>();\n",
    "        auto B_ptr = B.data_ptr<float>();\n",
    "        auto C_ptr = C.data_ptr<float>();\n",
    "        for (int i = 0; i < remain; i++) {\n",
    "            C_ptr[start + i] = A_ptr[start + i] + B_ptr[start + i];\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "// PyTorch绑定\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"elementwise_add_f32x4\", &elementwise_add_f32x4, \"Elementwise Add f32x4 (float4 vectorized)\");\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aae3b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/shaneyale/.cache/torch_extensions/py312_cu124 as PyTorch extensions root...\n",
      "The input conditions for extension module elementwise_fp32x4 have changed. Bumping to version 2 and re-building as elementwise_fp32x4_v2...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/shaneyale/.cache/torch_extensions/py312_cu124/elementwise_fp32x4/build.ninja...\n",
      "/home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module elementwise_fp32x4_v2...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] /usr/local/cuda-12.4/bin/nvcc --generate-dependencies-with-compile --dependency-output elementwise_fp32x4.cuda.o.d -DTORCH_EXTENSION_NAME=elementwise_fp32x4_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/include -isystem /home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/include/TH -isystem /home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.4/include -isystem /home/shaneyale/miniconda3/envs/cu124/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_89,code=compute_89 -gencode=arch=compute_89,code=sm_89 --compiler-options '-fPIC' -O3 --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -gencode arch=compute_89,code=sm_89 -std=c++17 -c /home/shaneyale/codes/cuda/easy/elementwise/elementwise_fp32x4.cu -o elementwise_fp32x4.cuda.o \n",
      "[2/2] c++ elementwise_fp32x4.cuda.o -shared -L/home/shaneyale/miniconda3/envs/cu124/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.4/lib64 -lcudart -o elementwise_fp32x4_v2.so\n",
      "------------------------------------------------------------\n",
      "S=1024, K=1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module elementwise_fp32x4_v2...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp32x4: [1.86473846, -0.87125796], mean time: 0.01842213 ms\n",
      "Result check: PASS\n",
      "------------------------------------------------------------\n",
      "S=1024, K=2048\n",
      "fp32x4: [-1.71387815, 0.54864913], mean time: 0.02921009 ms\n",
      "Result check: PASS\n",
      "------------------------------------------------------------\n",
      "S=1024, K=4096\n",
      "fp32x4: [-0.11894207, -1.35181189], mean time: 0.20104289 ms\n",
      "Result check: PASS\n",
      "------------------------------------------------------------\n",
      "S=2048, K=1024\n",
      "fp32x4: [1.36392868, 0.77033579], mean time: 0.01551175 ms\n",
      "Result check: PASS\n",
      "------------------------------------------------------------\n",
      "S=2048, K=2048\n",
      "fp32x4: [-0.62173986, 1.71409369], mean time: 0.20037961 ms\n",
      "Result check: PASS\n",
      "------------------------------------------------------------\n",
      "S=2048, K=4096\n",
      "fp32x4: [1.70619178, -2.45300078], mean time: 0.41244340 ms\n",
      "Result check: PASS\n",
      "------------------------------------------------------------\n",
      "S=4096, K=1024\n",
      "fp32x4: [-0.64906311, 0.39583477], mean time: 0.20041490 ms\n",
      "Result check: PASS\n",
      "------------------------------------------------------------\n",
      "S=4096, K=2048\n",
      "fp32x4: [0.80878097, 0.25631189], mean time: 0.41230583 ms\n",
      "Result check: PASS\n",
      "------------------------------------------------------------\n",
      "S=4096, K=4096\n",
      "fp32x4: [2.00575113, -0.28993881], mean time: 0.85782957 ms\n",
      "Result check: PASS\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# ===============================\n",
    "# Load the CUDA kernel (fp32x4)\n",
    "# ===============================\n",
    "lib = load(\n",
    "    name=\"elementwise_fp32x4\",\n",
    "    sources=[\"elementwise_fp32x4.cu\"],\n",
    "    extra_cuda_cflags=[\n",
    "        \"-O3\",\n",
    "        \"--expt-relaxed-constexpr\",\n",
    "        \"--expt-extended-lambda\",\n",
    "        \"--use_fast_math\",\n",
    "        \"-gencode\", \"arch=compute_89,code=sm_89\" \n",
    "    ],\n",
    "    extra_cflags=[\"-std=c++17\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Benchmark function\n",
    "# ===============================\n",
    "def run_benchmark(func, a, b, out=None, warmup=10, iters=1000):\n",
    "    if out is not None:\n",
    "        out.fill_(0)\n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        func(a, b, out)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start = time.time()\n",
    "    for _ in range(iters):\n",
    "        func(a, b, out)\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "\n",
    "    mean_time = (end - start) * 1000 / iters  # ms per iteration\n",
    "    out_val = out.flatten().detach().cpu().numpy().tolist()[:2]\n",
    "    out_val = [round(v, 8) for v in out_val]\n",
    "    print(f\"fp32x4: {out_val}, mean time: {mean_time:.8f} ms\")\n",
    "    return out, mean_time\n",
    "\n",
    "# ===============================\n",
    "# Test different sizes\n",
    "# ===============================\n",
    "Ss = [1024, 2048, 4096]\n",
    "Ks = [1024, 2048, 4096]\n",
    "\n",
    "for S in Ss:\n",
    "    for K in Ks:\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"S={S}, K={K}\")\n",
    "        a = torch.randn((S, K), device=\"cuda\", dtype=torch.float32).contiguous()\n",
    "        b = torch.randn((S, K), device=\"cuda\", dtype=torch.float32).contiguous()\n",
    "        c = torch.zeros_like(a)\n",
    "        \n",
    "        run_benchmark(lib.elementwise_add_f32x4, a, b, c)\n",
    "\n",
    "        # 验证结果正确性\n",
    "        if torch.allclose(a+b, c):\n",
    "            print(\"Result check: PASS\")\n",
    "        else:\n",
    "            print(\"Result check: FAIL\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
